---
title: "Mortality Models"
author: "J Camac"
date: "27 July 2014"
output: pdf_document
---

Mortality model
========================================================

**Summary**
Below outlines the code used for a linear cloglog mortality model that estimates the probability an individual will die by the next census (t+1) as a function of wood density (measured at species level) and either a size (e.g. dbh or basal area) or their associated growth rates (either incremental growth or relative growth rate - measured at individual level).

*Data*
This model uses BCI data for single stemmed plants that are alive for at least two censuses - which is required to estimate a growth rate.
Furthermore, individuals that return from the dead have also been removed from the dataset as well as individuals that are missed in one or more censuses. Note the latter two cases involves few individuals. 
Observations in 1990 are treated as baseline. Thus mortality is not estimated in this year because there is no record of growth rate and only individuals alive in that year are used. 
Observed deaths in 2010 are included. However, model does not make predictions based on 2010 growth rates for 2015 mortality. *This can be done at a later stage.*
Because this model predicts future mortality, the model only uses observations of when the individual is alive. Thus census observations of deaths become redunant and are removed from the final dataset (as this information is incoporated into the column dead.next.census).
Lastly only species that contain trait values are used within this model.

In total there are:
240 species
164,472 individuals
391,267 observations
For complete break down of observations removed see make.bci.mainstem.R.

*Covariate transformations*
In this model the covariates have been centered on their means. Thus the intercept is interpreted as the average probability an individual of average wood density and growth rate will die in the next census. The covariate effects are the additional effects when either covariate is above or below their mean. *Note* signs of covariates invert when estimating covariate values below the mean.
Covariates have also been scaled by two standard deviations so that effect sizes can be compared on standardised scale - see Gelman & Hill 2007. 
*Potential issue* some of the covariate data is skewed and thus centering and scaling will be biased. dbh, basal.area and their incremental growth rates cannot be log or sqrt transfromed as negative growth is observed. Will consider other transformations.

*Model assumptions*
* Hazard rate is constant - i.e. Probability of dying is not adjusted by the probability of dying in the previous census.
* The model assumes a linear, multiplicative relationship between traits and growth rate covariates.
* Models incoporating RGR implicilty include size effects.


```{r Data prep, echo=FALSE}
library('dplyr')
library('R2jags')
load('../data_BCI/output/bci.mainstem.Rdata')
bci.traits <- read.csv('../data_BCI/rawdata/traits/BCI_traits_20101220.csv')
names(bci.traits) <- tolower(names(bci.traits)) # lowers trait column names for merging
bci.traits$sp <- tolower(bci.traits$sp) # lowers species code names for merging

bci.mainstem <- merge(bci.mainstem,bci.traits[,c('sp','sg100c_avg')],by = 'sp') #only uses species trait data exists for.
rm(bci.traits) # no longer needed

bci.mainstem<- bci.mainstem %.%
  filter(!is.na(census.interval) & !is.na(sg100c_avg) & !is.na(dead.next.census) & census.interval< 6) %.%
  mutate(sp.id = as.numeric(factor(sp), as.character(unique(sp))))
```

```{r Linear cloglog mortality model, echo=FALSE}
  data <- list(
  n.obs = nrow(bci.mainstem),
  n.spp = length(unique(bci.mainstem$sp)),
  SPP = as.numeric(factor(bci.mainstem$sp), as.character(unique(bci.mainstem$sp))),
  wd = scale(unique(bci.mainstem[c('sp', 'sg100c_avg')])[,'sg100c_avg'])[,1]/2,
  dbh = scale(bci.mainstem$dbh)[,1]/2,
  dbh.gr = scale(bci.mainstem$dbh.gr)[,1]/2,
  dbh.rgr = scale(bci.mainstem$dbh.rgr)[,1]/2,
  basal.area = scale(bci.mainstem$basal.area)[,1]/2,
  basal.gr = scale(bci.mainstem$basal.area.gr)[,1]/2,
  basal.rgr = scale(bci.mainstem$basal.area.rgr)[,1]/2,
  ln.census = scale(log(bci.mainstem$census.interval))[,1]/2,
  died = bci.mainstem$dead.next.census)

# Note this runs multiple models with each loop iteration changing the covariates used
models <- c('dbh', 'dbh.gr', 'dbh.rgr', 'basal.area', 'basal.gr','basal.rgr')
nmodels <- length(models)
for (i in 1:nmodels) {
cat(sprintf('
  model {
    for(it in 1:n.obs) {
      
      died[it] ~ dbern(p[it])
      cloglog(p[it]) <- Alpha[SPP[it]] + b.2 * %1$s[it] + b.3 * (wd[SPP[it]] * %1$s[it]) + ln.census[it]
      }
    
    for (j in 1:n.spp) {
      Alpha[j] <- GM + b.1 * wd[j]
      }
    
    #Priors
    
    GM ~ dnorm(0, 0.0001) # Intercept
    b.1 ~ dnorm(0, 0.0001) # Wood Density effect
    b.2 ~ dnorm(0, 0.0001) # Growth/size effect
    b.3 ~ dnorm(0, 0.0001) # Wood density x growth/size effect
  
    }', names(data)[which(names(data) %in% models)][i]),
    file=(tmp <- tempfile()))
  
  assign(sprintf('%s.mod', models[i]), 
         jags.parallel(model.file = tmp,
                 n.iter = 4000,
                 data= names(data), 
                 envir=list2env(data),
                 inits = NULL,
                 parameters.to.save = c('GM','b.1','b.2','b.3')))
  }
```  
