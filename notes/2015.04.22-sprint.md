# Sprint 5: 2015.04.07-2015.04.17

Most of this sprint has focused on implementing cross validation, estimating log likelihoods and implementing stan models in the cluster

##TASKS
**Daniel & James**

*In collaboration with NICTA examine ways in which we can conduct cross validation on a dataset that contains individuals repeatedly measured through time, and thus are not independent.**

Conclusion:

We will randomly select one observation per individual tree. From here we then sort the data by species and assign individuals into 1 to 10 k-folds. This removes the problem of non-independence, allows all censuses to be used and can be readily applied to cross validation.

*Should we approach our hypotheses about wood density effects on mortality as a series of models or as a single model with emphasis on effect sizes*

Model selection approach:
- Treat hypotheses as a series of models of various complexity (e.g. Wood density on a_log, b_log, c_log, or two of of them, or all three) and compare using Bayes factors of WAIC.
- This approach will provide the 'best' and most parsimonious model.

Single model approach:
- Hypotheses are not a series of models, but rather about effect sizes on a_log, b_log and c_log.
- This approach focuses on the relative effect sizes and uncertainty associated with each parameter. It might also be possible to examine the proportion of variance explained by each using a variance components analysis.

Conclusion:
After careful thought, I believe there is scope to conduct both methods using the following framework:
1. We have a single model exp(a - exp(b) * GR) + exp(c). This model encapsulates both growth dependent and growth independent processes of mortality. Because plants can die due to either cause, it does not make biological sense to examine simpler forms of this model (e.g. exp(a- exp(b) * GR) vs exp(c) vs exp(a - exp(b) * GR) + exp(c)), as the simplifed models assume mortality can either be growth dependent or it's effect isn't partitioned from growth independent sources.

2. Using this 'full model' with wood density & random effects for census and species on each parameter, we then examine which growth rate measure has the highest fitted log likelihood and highest cross validation log likelihood. Here model selection is important as we do not have underlying hypothesis why one growth measure should do better than another. The reason we fit wood density to a, b and c in a single model is because it best reflects current knowledge of how wood density influences plant mortality (i.e. wood density has been shown to affect growth dependent (shade tolerance) and growth independent processes (mechanical durability)).

3. Once a growth measure has been chosen as 'best', we then use the fitted model to examine how wood density influences a, b and c. This will involve examining effect sizes and uncertainty. It may also be possible to conduct a variance components analysis to assess the proportion of explained variance wood density explains on each.

4. In addition to conducting a variance components analysis, we could use a model selection approach (i.e. WAIC or Bayes factors) to examine wood density effects on:
    1: a
    2: b
    3: c
    4: a,b
    5: a,c
    6: b,c
    7: a,b,c
    
I think both the variance components analysis and model selection approach provide slightly different information. The variance components analysis/effect sizes is useful in that it provides insights into where wood density provides the greatest effect (e.g. is it the intercept of the growth-mortality relationship, or is it predominately in the growth independent constant?). By contrast, the model selection approach selects the best/most parsimonous model, without explicit consideration of the magnitude or uncertainty of wood density effects on each parameter. However, the model selection approach will provide insights into which (if any) wood density effects can be dropped in order to use a more parsimonous model that can be applied to smaller datasets.


**Rich**
1. Helped set up the AWS cluster and has begun implementing models on AWS cluster using Redis and Docker. This approach should be more scalable and applicable then our current use of mclapply and will be of use to other projects associated with the SIEF grant. *Mostly done?*


**James**
1. Write simplified functional forms of stan mortality model (i.e. exp(c_log), exp(a_log - exp(b_log)* GR), exp(a_log - exp(b_log)* GR) + exp(c_log)). *DONE*
2. Add increasing complexity to functional forms (i.e. constant models, species random effect,trait only effects, species random effect + trait effect) *DONE*
3. Add random effect of census *Needs to be checked*

**Daniel**
1. Get access to AWS cluster and work with James in getting stan models to work with remake. *Done*
2. Get models and cross validation to run on AWS cluster using mclapply. Specifically, get all variants of the species random effect models (120 models; 4 GR * 3 functional forms * 10 folds) to run in a cross validation framework *Done*
3. Write function to estimate log likelihood of model given held out data. *Done but still having issues with replicating the exact log likelihood produced in stan*


##Where to next?
1. Examine fitted models for determining growth rate measure (needs to be rerun with census and species random effects + trait effect - but should broadly provide the same answer which can be used to plan subsequent model fits).
2. Write models with and without wood density on each of a, b, and c.
3. Check that census random effect version works
4. Some models are running into initial sampling problems. As a group we need to discuss using more informative priors. My suggestion is to use cauchy priors.
5. Log likelihoods that are to be compared should be based on predictive fit. i.e. We should not be fitting the estimated random effects, rather we should be fitting the mean effects when predicting mortality. That is, for a average species the effect of wood density is.... Note, this is not the same as simply fitting the model with no random effects, as this approach does not estimate the effect of wood density with consideration of unexplained species variation... and consequently, in the simple linear regression the estimated effects of wood density will be overly precise.
6. Check that we can accurately estimate log likelihoods
7. Check whether we lose much data by using the open source wright et al 2010 trait dataset as opposed to the 2012 BCI trait dataset (which is not currently open source).
8. Rerun models based on new structure: Full model > model selection for growth rate measure > Examine effect sizes and variance components analysis > Model selection to determine most parsimonous model
   
   
   Plot 1: Coefficient plot of full model for best growth measure; 
   Plot 2: Simulated curves with raw data plotted (both hazard and survival?); 
   Plot 3: Estimated census effects (do they differ?).
   Table 1: Showing variance of random effects (useful for determining where most unexplained variation occurs (species or census?))
   Table 2: Growth rate model selection table (cross val & fitted lps)
   Table 3: Wood density model selection table (WAIC?)
   
   