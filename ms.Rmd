---
title: "Wood density and the growth-dependent & independent mortality"
author: "J S Camac"
date: "17 August 2015"
output: pdf_document
csl: downloads/style.csl
bibliography: data/refs.bib
---

# Introduction

Understanding who lives or dies, and why, is critical to accurately predict population, community and ecosystem dynamics [@Hawkes:2000ib; @McDowell:2011dr]. In plants, the carbon budget is considered a fundamental determinant of mortality, such that if rates of carbon assimilation are low or become insufficient for growth, an individual is more susceptible to dying [@Peet:1987va; @Hawkes:2000ib; @Keane:2001db; @McDowell:2011dr]. Empirical studies examining intraspecific mortality have broadly supported this assumption, with many finding that the probability a plant dies decreases exponentially with increasing growth rates [@Buchman:1983vx; @Kobe:1997vy; @Wyckoff:2002ul; @Mantgem:2003dw]. Consequently, most dynamic vegetation models, including tree gap models [@Botkin:1972ii; @Keane:2001db], SORTIE (REF) and Dynamic Global Vegetation Models (DGVM's; @Woodward:2004ft) use the growth-mortality relationship as the predominant, and in some cases, only mechanism describing plant mortality [@Hawkes:2000ib; @McDowell:2011dr]. However, despite empirical evidence highlighting that mortality rates vary considerably between species, many of  dynamic model assume the shape of the growth-mortality relationship is constant across species.

Multispecies studies have shown that species with intrinsically faster growth rates exhibit higher mortality rates relative to slower growing species [@Poorter:2008iu; @Wright:2010fl], and thus, species have different mortality rates. Ecologists have attempted to explain this variation between species by using functional traits - attributes of species that are thought to influence vital rates (i.e. survival, growth and reproduction) and ultimately fittness [@Ackerly:2003eb; @Poorter:2008iu]. Wood density, the biomass invested per unit wood volume, is one such functional trait that has been shown to be negatively correlated with tropical tree mortality [@Chave:2009iy; @Nascimento:2005wx; @King:2006he; @AlvarezClare:2007gv; @Chao:2008hg; @Poorter:2008iu; @Kraft:2010kq; @Wright:2010fl]. The underlying mechanism of this relationship is thought to be due to denser wood conveying enhanced physical resistance to stem breakage [@Putz:1983wu; @vanGelder:2006ex; @Chave:2009iy], wood herbivory (NEED REF), embolism [@Hacke:2001kj, @Jacobsen:2005fx, @Preston:2006jn] and fungal and pathogen attack [@Augspurger:1984wx]. However, research has also indicated that wood density may increase survival associated with growth-dependent mortality. For example, species with high wood density are often more shade tolerant, and thus exhibit a lower probability of dying at low growth rates [e.g. @vanGelder:2006ex]. Studies have also shown that wood density may reduce drought induced carbon starvation [@McDowell:2011dr] and the susceptibility of ‘stressed’ individuals to opportunistic invertebrate, pathogen or fungal attack [@Augspurger:1984wx; @McDowell:2011dr].

To date, most plant mortality research can be broadly seperated into two research focuses: 1) those that focus on estimating individual mortality within species [e.g. @Kobe:1997vy]; and 2) those that use suites of species to examine correlations between functional traits and population level estimates of plant mortality (e.g. Wright et al 2010, Chave et al 2004). However, little crossover of ideas exists between these two research focuses. This is despite both individual and species level variation likely being required to build a general model of plant mortality. Furthermore, in both research focuses, the measure of growth rate used to examine the growth-mortality relationship appears to be based on researcher preference with little consideration given as to whether an area or diameter based growth rate is a better predictor of plant mortality.

In this study, we reconcile both research focuses by fitting 15-years of growth and mortality data from 203 tropical rainforest species encompassing more than 180,000 individuals in a Bayesian hierarchical model that explicitly accounts for variability observed both within and between species. Specifically, our model uses the negative exponential growth-mortalty relationship commonly observed and fitted in intraspecific studies and extends this by both adding a growth-independent baseline hazard, and by allowing each parameter to vary as a function of species level traits (in this case wood density).

We then use this model, coupled with k-fold cross validation to address four questions:

1. Which growth measure is most predictive of plant mortality?
2. Does wood density influence both the growth-mortality trade-off and growth independent mortality?
3. Does the effects of wood density vary with growth measure? and
4. What is the most parsimonious mortality model in terms of wood density effects?

\pagebreak

# Methods

## Data

We fit our model using growth and survival data collected from a 50-ha tropical rainforest plot on Barro Colorado Island (BCI), Panama (9.15&deg;N, 79.85&deg;W). The plot is predominately undisturbed old-growth rainforest (48-ha), but 2 ha is of 100 year old secondary rainforest (REF). The climate at Barro Colorado Island is warm throughout the year with a mean annual temperature of 26.1&deg;. Rainfall is seasonal with most of the 2500 mm falling between April and November (REF). The island is managed exclusively for field research by the Smithsonian Tropical Research Institute (STRI), which was granted long-term custodianship over the island by Panama's Environmental Authority. STRI have permission to establish the 50-ha plot as a permanent census in 1980. Detailed descriptions on the flora, fauna, geology and climate of BCI can be found in (REFS).

Within the 50-ha BCI plot the diameter at breast height and survival status of all free-standing woody plants with a diameter greater or equal to 1 cm were recorded in 1981-1983, 1985, and every 5 years thereafter [@BarroColoradofores:2012up]. For the purpose of this study, we discarded data collected in 1982 and 1985 censuses because diameter measurements were rounded to the nearest 5 mm whereas later censuses were rounded to the nearest millimetre. We excluded individual trees that did not survive at least two censuses (two being required to estimate growth rates), were not consistently measured at 1.3 m, were multi-stemmed, or had re-sprouted or 'returned from the dead'. We also discarded species that do not exhibit secondary growth (e.g. palms and ferns), contained fewer than 10 individuals or did not contain a wood density estimate were also discarded. Extreme outliers: stems which grew more than 5 cm/yr or shrunk more than 25\% of their initial dbh were also removed. Lastly, in order to reduce computational requirements we randomly selected one observation per individual. In total, 180,500 individuals of 203 species were included in our analysis.

Wood density for each species was estimated by coring trees located within 15 km of the BCI plot because increment borers are prohibited within the plot [@Wright:2010fl]. Cores were broken into pieces, each 5 cm long and specific gravity of each piece was determined by oven drying (100&deg;C) and dividing by the fresh volume (as measured by water displacement). Wood density was then calculated as an area-weighted average (g cm$^-3$), where area refers to the annulus represented by each piece, assuming a circular trunk.

## Measurement error and calculating true growth

Observed dbh measurements are likely to include measurement error that can manifest into biologically unrealistic scenarios of negative growth and ultimately erroneous conclusions. We estimate this measurement error using another BCI dataset consisting of 1562 randomly selected trees remeasured within 30 days (see XXXX). Assuming that no growth occurred between observations, we estimated the standard deviation of measurement error (and its uncertainty) by sampling the discrepencies for each individual, $i$, as random realisations from a normal distribution centered on zero with a standard deviation, $\sigma_{error}$, which describes the variation in error:

\begin{equation} \label{eq:error_model}
discepency_i \sim \textrm{N}(0, \sigma_{error}). \end{equation}


$\sigma_{error}$ was estimated to be 0.75 (sd = 0.01), meaning that 95% of the measurement errors were within +/- 1.47 cm of the true diameter. We then used this standard deviation of 0.75 in another Bayesian model to estimate the initial and final true dbh (i.e. observation - measurement error) for each individual. This was done by first modelling the observed initial ($dbh_{t-1}$) and final dbh ($dbh_{t}$) of each individual, $i$, as random realisations from a normal distribution centered on their respective true estimates (i.e. $\widehat{dbh1_i}$ and $\widehat{dbh2_i}$), with a standard deviation of 0.75 (i.e. $\sigma_{error}$).

\begin{equation} \label{eq:obs dbh1}
dbh_{i,t-1} \sim \textrm{N}(\widehat{dbh_{i,t-1}}, 0.75). \end{equation}

\begin{equation} \label{eq:obs dbh2}
dbh_{i,t} \sim \textrm{N}(\widehat{dbh_{i,t}}, 0.75). \end{equation}

$\widehat{dbh_{i,t-1}}$ was estimated from a lognormal distribution:

\begin{equation} \label{eq:true dbh1}
\widehat{dbh_{i,t-1}} \sim \textrm{lognormal}(\mu_{dbh_{i,t-1}}, \sigma_{dbh_{t-1}}), \end{equation}

where, $\mu_{dbh_{t-1}}$ defines the log of the grand mean initial dbh, and $\sigma_{dbh_{t-1}}$ defines that variation around that mean. $\mu_{dbh_{t-1}}$ was estimated using a normal prior $\textrm{N}(0, 2.5)$, which represents our prior expectation that the mean initial dbh would likely occur between 0 and 148 cm. $\sigma_{dbh{t-1}}$ was estimated using a weakly informative half cauchy prior centered on zero and a scale parameter of 2.5.

$\widehat{dbh_{i,t}}$ was estimated as:

\begin{equation} \label{eq:true dbh2}
\widehat{dbh_{i,t-1}} + R_i \, T_i, \end{equation}

where, $R_i$ represents the true annual increase in dbh (cm) and $T_i$ represents the number of days between measurements divided by 365.25. We modelled $R_i$ as a random realisation from a half normal distribution centered on zero and a standard deviation of 5, conveying our prior expectation that annual increases in dbh would be between 0 and 10 cm/yr. We used a half normal as it ensures dbh growth cannot be negative. 

\begin{equation} \label{eq:obs dbh increment}
R_i \sim \textrm{N+}(0, 5). \end{equation}.

For computational efficiency, we used the posterior means of $\dbh_t-1$, $dbh_t$ to calculate:

dbh growth rate:  $\frac{\widehat{dbh_{i,t}} - \widehat{dbh_{i,t-1}}}{T_i}$ 
basal area growth rate: $\frac{(\pi \, 0.25 \,\widehat{dbh_{i,t}}) - (\pi \, 0.25 \, \widehat{dbh_{i,t-1}})}{T_i}$.

These estimates of individual post growth were then used in the subsequent mortality analysis.


## Mortality model description

We extend a common functional form used to model the growth-mortality relationship [e.g. @Kobe:1995] to include a growth-independent baseline hazard, such that the functional form becomes
\begin{equation} \label{eq:lambda}
\lambda = \overbrace{\alpha \, \exp\left(-\beta \, G \right)}^{\text{Growth dependent}} + \overbrace{\gamma}^{\text{Growth independent}}
\end{equation}

By adding $\gamma$ we allow the functional form to asymptote at a mortality rate above zero, which allows for deaths to occur at high growth rates that would otherwise not be accounted for. The advantage of this functional form is that all parameters are biologically interpretable. $\alpha$ represents the intercept and biologically defines the sum of mortality rate at zero growth and baseline hazard. $\beta$ represents the function's decay, or in other words, the sensitivity of mortality rates to change with growth rate. $\gamma$, as stated before, represents asymptote and is referred to as the baseline hazard. $\alpha$ and $\beta$ are intrinsically linked to growth rate, and as such are growth-dependent parameters. By contrast, $\gamma$ accounts for mortality not related to growth caused by stachastic events (e.g. windfall, fire) and consequently is a growth-independent parameter.

To incorporate varability in species mortality rates, we add allowed $\alpha$, $\beta$ and $\gamma$ to vary by wood density - a species trait thought to encapsulate a species growth strategy (REFs). In addition to this, we also allowed the three parameters to vary by species ID, by doing this, we effectively allow the model to partition variation explained by wood density from the residual variation explained by species and individual levels [@Gelman:2007te; @Kery:2010tp].

## Bayesian Hierarchical Model implementation

We have binary observations $y_i$ (0 = alive, 1 = dead) for each individual $i$. We also have a hypothesised underlying mortality model giving an instantaneous rate of mortality (or hazard) per unit time (eq. \ref{eq:lambda}). Based on standard demographic theory, the probability of death for individual $i$ across the census interval $t_1\rightarrow t_2$ is
\begin{equation} \label{eq:pred_model}
p_{i,t_1\rightarrow t_2} = 1 - \exp\left(\Lambda_{i,t_1\rightarrow t_2} \right),
\end{equation}
where $\Lambda_{i,t} = \int_{t1}^{t2} \lambda(t) \, dt$ is the cumulative hazard for that individual. Given estimates of $\lambda$, one can model the observations as a random realization from a Bernoulli distribution based on the probability that individual $i$ would die, $p_{i,t_1\rightarrow t_2}$:
\begin{equation} \label{eq:obs_model}
y_{i,t_1\rightarrow t_2} \sim \textrm{bernoulli}(p_{i,t_1\rightarrow t_2}).\end{equation}

Our first hypothesis is that the cumulative hazard rate within a census interval is reasonably approximated by the true growth rate (estimated as above) in the previous interval
\begin{equation} \label{eq:hazard2}
\Lambda_{i,t} \approx (t_2-t_1) \lambda (t_1).
\end{equation}

Second, we assume that the three parameters of eq. \ref{eq:lambda} are influenced by 3 factors: wood density, species ID and census year. Because wood density is measured at species-level, its effect is logically a subset of the overall species level effect.

To account for species strategy and partition variance into explained and unexplained at the observation and species level we modelled $\alpha$, $\beta$ and $\gamma$ via the following submodels:
\begin{equation} \label{eq:submodels}
\alpha_s = \alpha_{0,s} \, \rho_s^{\alpha_{1}},
\beta_s = \beta_{0,s} \, \rho_s^{\alpha_{1}},
\gamma_s = \gamma_{0,s} \, \rho_s^{\alpha_{1}}.
\end{equation}
Here $\alpha_{0,s}$,$\beta_{0,s}$,$\gamma_{0,s}$ represents the species-level random effect of species $s$ within which individual $i$ belongs and $\alpha_{1}$,$\beta_{1}$,$\gamma_{1}$ represents the effect of standardised wood density $\rho_s$ on $\alpha$,$\beta$ and $\gamma$, respectively. The form of submodels in eq \ref{eq:submodels} ensures that parameters remain positive. On a log scale these submodels \ref{eq:submodels} become additive (e.g. $\log \alpha_s = \log \alpha_{0,s} + \alpha_{1} \, \log \rho_s$, equivalent to a classic proportional hazards model widely used in survival analysis. We standardised wood density via the transformation: $\rho_s = \rho/0.6$), an operation which is equivalent to centering a log-transformed variable at a density of 0.6 g cm${^-3}$. This was done for easier interpretation, allowing $\alpha_{1,t}$ to be interpreted as the hazard for a species with wood density of 0.6cm${^-3}$ (as opposed to species responses at $\rho$ = 0; an biologically unrealistic state for a woody plant) [@Gelman:2007te].

The species random effects on each parameter were assumed to be a random realistation from a log-normal distribution:
\begin{equation} \label{eq:RE_species}
\alpha_{0,s} \sim \text{log-normal}\left(\mu_{\log \alpha_0}, \sigma_{\log \alpha_0}\right),\\
\beta_{0,s} \sim \text{log-normal}\left(\mu_{\log \beta_0}, \sigma_{\log \beta_0}\right),\\
\alpha_{0,s} \sim \text{log-normal}\left(\mu_{\log \gamma_0}, \sigma_{\log \gamma_0}\right),\\
\end{equation}

We estimated $\mu_{\log \alpha_0}$, the average log species effect at zero growth using the following informative prior:

\begin{equation} \label{eq:alpha0}
\mu_{\log \alpha_{0}} \sim \text{normal}\left(-0.61, 0.57\right).\\
\end{equation}

This distribution was derived from data published by Kobe et al 1995, who estimated the probability of dying at zero growth/2.5 years for juveniles of 10 hardwood species across north-eastern America. Note that these data are also used to define the growth-mortality relationship in SORTIE-ND a spatially explicit forest dynamic model (REF). We converted these probabilities/2.5 years to instantaneous hazard rates by using $\lambda = -\log(1 - Pr(Death))/2.5$, and then log transforming these values and calculating the mean (i.e. -0.61) and standard error (i.e. 0.57). Based on this prior, we assumed that the average probability of death was between 16 and 81% with the central tendency being 42%/yr.

EXAMPLE:
```{r}
# white ash
median(c(0.999,0.55,0.998))
# sugar maple
median(c(0.998, 0.148, 0.999)
# American beech
median(c(0.014,0.12))
# Yellow Birch
median(c(0.555,0.995))
# Red Maple
0.99
# Eastern hemlock
0.077
# Black cherry
0.998
# White oak
0.82
# Red oak
0.985
#White pine
0.268

# mean species instantaneous hazard rate (used medians for species with multiple measurements)
M <- c(0.998,0.998,0.067,0.775,0.99,0.077,0.998,0.82,0.985, 0.268)
k = - log(1-M)/2.5
mean(log(k))

# standard error
sd(log(k))/sqrt(10)
```

Similarly, we used prior information from Korning and Balslev et al (1994) to estimate, $\mu_{\log \gamma_0$, the average log species effect on $\gamma$, such that:

\begin{equation} \label{eq:gamma0}
\mu_{\log \gamma_0 \sim \text{normal}\left(-3.86, 0.2\right).\\
\end{equation}

This distribution was estimated the same way as in $\mu_{\log \alpha_0$. That is we converted annual mortality probabilities of 17 tropical lowland tree species in Amazonian Ecuador that encompass the range of forest structure (i.e. understory species, small stemmed midcanopy species, large stemmed midcanopy species and canopy species) to instantaneous hazard rate, and then log transformed these and calculated the mean (i.e. -3.86) and standard error (i.e. 0.2) These annual mortality estimates were averaged over size and growth rates, and as such are likely to be an overestimate of the true value of $\gamma$. However, based on this prior, we assume that the average growth-independent species mortality will be 2.1%/yr with 95% of species growth-independent probabilities of occuring between 1.4% and 3.1%.

EXAMPLE:
```{r}
M <- c(8.6,1.8,0.6,3.8,0.9,0.7,4.3,4.7,1.8,6.4,0.8,1,2,1.8,3.7,2.2,2.2)/100 # From Korning Baslev 1994
k = - log(1-M)/1
mean(log(k))

# standard error
sd(log(k))/sqrt(length(M)) 
```

As we fitted our model to two growth measures (i.e. dbh growth and basal area growth), we used weak normal priors on $\mu_{\log \beta_0$, the average log species decay rate where:


\begin{equation} \label{eq:beta_0}
\mu_{\log \beta_0 \sim \text{normal}\left(0, 5\right).\\
\end{equation}

This prior, regardless of growth measure, spans a range such that the decay of the negative exponential can be rapid or is flat, suggesting no effect of growth rate on mortality rates.


The effect of wood density on all three parameters (i.e. $\alpha_1$, $\beta_1$ and $\gamma_1$) were given normal priors centered on zero and a standard deviation of 5:

\begin{equation} \label{eq:a2b2c2}
\alpha_2, \beta_2, \gamma_2 \sim \text{normal}\left(0, 5\right).\\
\end{equation}


Standard deviations associated with species random effects (i.e. $\sigma_{a0}$, $\sigma_{b0}$, $\sigma_{c0}$), were all sampled from positive half cauchy priors with centered on zero with a scale of 2.5.

\begin{equation} \label{eq:a1b1c1,s}
\sigma_{a0},\sigma_{b0},\sigma_{c0} \sim \text{cauchy+}\left(0, 2.5\right).\\
\end{equation}


## Cross validation

To implement K-fold cross-validation in Stan we need to repeatedly fit the model to one subset of the data and use it to predict the held-out set. The way we recommend setting this up is to do the partitioning in R (or Python, or whatever data-processing environment is being used) and then pass the training data and held-out data to Stan in two pieces. With linear regression, for example, we start with the Stan model on page 4, passing in the training data as N, y, X and the held-out set as N holdout, y holdout, X holdout, with the data block augmented accordingly. We then keep the data block and model as is, and just alter the generated quantities block to operate on the held-out data.

For a model $M$, training data $D$, and test data $D^*$, we want to calculate

$$p(D^* | D,M) = \int p(\theta | D, M) \, p(D^* | \theta, M) \, d \theta$$

The distribution $p(\theta | D, M)$ gives the likelihood of parameter values $\theta$ given the training data (obtained from fitting the model) and is calculated (via Bayes Rule) as

$$p(\theta | D, M) = \frac{p(\theta) \, p(D|\theta)}{p(D)}.$$

$p(D^* | \theta, M)$ is then the probability of observing the test data given a particular parameter set (i.e the likelihood of the data) and is calculated as
$$p(D^* | \theta, M) = \prod_i p(D^*_i | \theta, M).$$

Log-likelihoods are easier to deal with and so we have

$$\log p(D^* | \theta, M) = \sum_i \log p(D^*_i | \theta, M).$$

Aki \& Gelman 2014 provide methods for estimating "an importance-sampling approximated LOO directly using the log-likelihood evaluated at the posterior simulations of the parameter values"


We fitted these models using package rstan version 2.7 [@StanDevelopmentTeam:2015uz]. Three independent chains were executed and in all cases modelled parameters converged within 1000 iterations. Convergence was assessed through both visual inspection of chains and reference to the Brooks-Gelman-Rubin convergence diagnostic [@Gelman:1992ts; @Brooks:1998ju]. After discarding the first 1000 iterations as ‘burn in’, a further 1000 iterations were taken from the joint posterior. The residuals of the model were checked graphically against predictions with none showing any systematic pattern or severe heteroscedasticity.



\pagebreak

## STAN MODEL
```{r, eval=FALSE}
data {
  int<lower=1> n_obs;
  int<lower=1> n_spp;
  int<lower=1> spp[n_obs];
  int<lower=0, upper=1> y[n_obs];
  vector[n_obs] census_length;
  vector[n_obs] growth_dt;
  vector[n_spp] rho_c;
  
  // Held out data
  int<lower=1> n_obs_heldout;
  int<lower=1> n_spp_heldout;
  int<lower=1> spp_heldout[n_obs_heldout];
  int<lower=0, upper=1> y_heldout[n_obs_heldout];
  vector[n_obs_heldout] census_length_heldout;
  vector[n_obs_heldout] growth_dt_heldout;
  vector[n_spp_heldout] rho_c_heldout;
}

parameters { 
  
  // Mortality model parameters
  real raw_log_a0[n_spp];
  real mu_log_a0;
  real<lower=0> sigma_log_a0;
  
  real raw_log_b0[n_spp];
  real mu_log_b0;
  real<lower=0> sigma_log_b0;
  
  real raw_log_c0[n_spp];
  real mu_log_c0;
  real<lower=0> sigma_log_c0;
  
  
  
  
}

model {
  
  // Declaring mortality parameters
  real alpha;
  real a0[n_spp];
  
  real beta;
  real b0[n_spp];
  
  real gamma;
  real c0[n_spp];
  
  real cumulative_hazard;
  
  // Calculating species random effects
  for (s in 1:n_spp) {
    a0[s] <- exp(raw_log_a0[s] * sigma_log_a0 + mu_log_a0); // e.g. implies lognormal(mu_log_a0, sigma_log_a0)
    b0[s] <- exp(raw_log_b0[s] * sigma_log_b0 + mu_log_b0);
    c0[s] <- exp(raw_log_c0[s] * sigma_log_c0 + mu_log_c0);
  }
  
  for (i in 1:n_obs) {
    // Calculating mortality parameters
    alpha <- a0[spp[i]]; 
    beta <- b0[spp[i]]; 
    gamma <- c0[spp[i]];
    
    // Likelihood for hazard model
    cumulative_hazard <- -census_length[i] * (alpha * exp(-beta * growth_dt[i]) + gamma);
    
    if (y[i] == 0) {
      increment_log_prob(cumulative_hazard);
    } else {
      increment_log_prob(log1m_exp(cumulative_hazard));
    }
  }
  
  // Priors
  
  //Mortality model priors
  raw_log_a0 ~ normal(0,1);
  mu_log_a0 ~ normal(-0.61, 0.57);
  sigma_log_a0 ~ cauchy(0, 2.5);
  
  raw_log_b0 ~ normal(0, 1);
  mu_log_b0 ~ normal(0, 5);
  sigma_log_b0 ~ cauchy(0, 2.5);
  
  raw_log_c0 ~ normal(0, 1);
  mu_log_c0 ~ normal(-3.86, 0.2);
  sigma_log_c0 ~ cauchy(0, 2.5);
  
  
  
  
}

generated quantities {
  
  // Declaring fitted parameters
  real a0[n_spp];
  real b0[n_spp];
  real c0[n_spp];
  
  real alpha_fit;
  real beta_fit;
  real gamma_fit;
  
  real cumulative_hazard_fit;
  real log_lik_fit;
  real sum_log_lik_fit;
  
  // Declaring heldout parameters
  real alpha_heldout;
  real beta_heldout;
  real gamma_heldout;
  
  real cumulative_hazard_heldout;
  real log_lik_heldout[n_obs_heldout];
  real sum_log_lik_heldout;
  
  // Initialization of summed log likelihoods
  sum_log_lik_fit <- 0;
  sum_log_lik_heldout <- 0;
  
  // recalulate species random effects
  for (s in 1:n_spp) {
    a0[s] <- exp(raw_log_a0[s] * sigma_log_a0 + mu_log_a0);
    b0[s] <- exp(raw_log_b0[s] * sigma_log_b0 + mu_log_b0);
    c0[s] <- exp(raw_log_c0[s] * sigma_log_c0 + mu_log_c0);
  }
  
  
  // log likelihood for fitted model
  for (i in 1:n_obs) {
    alpha_fit <- a0[spp[i]]; 
    beta_fit <- b0[spp[i]]; 
    gamma_fit <- c0[spp[i]];
    
    cumulative_hazard_fit <- -census_length[i] * (alpha_fit * exp(-beta_fit * growth_dt[i]) + gamma_fit);
    
    if (y[i] == 0) {
      log_lik_fit <- cumulative_hazard_fit;
    }
    else {
      log_lik_fit <- log1m_exp(cumulative_hazard_fit);
    }
    sum_log_lik_fit <- sum_log_lik_fit + log_lik_fit;
  }
  
  // log likelihood for held out data
  for (j in 1:n_obs_heldout) {
    alpha_heldout <- a0[spp_heldout[j]]; 
    beta_heldout <- b0[spp_heldout[j]]; 
    gamma_heldout <- c0[spp_heldout[j]];
    
    cumulative_hazard_heldout <- -census_length_heldout[j] * (alpha_heldout * exp(-beta_heldout * growth_dt_heldout[j]) + gamma_heldout);
    
    if (y_heldout[j] == 0) {
      log_lik_heldout[j] <- cumulative_hazard_heldout;
    }
    else {
      log_lik_heldout[j] <- log1m_exp(cumulative_hazard_heldout);
    }
    sum_log_lik_heldout <- sum_log_lik_heldout + log_lik_heldout[j];
  }
} 
```


\pagebreak

# OTHER STUFF
To optimise calculations in stan, we tried to minimise the number of logarithmic and exponential transformations, resulting in the following algorithm:

1. Estimate $\alpha, \beta, \gamma$
  - Draw a number from standard lognormal distribution with location ($\mu$) and scale ($\sigma$) parameters being the mean and sd of logged variable. Note that this draw returns variable on non-logged axes, even though location and scale are specified on logged axes. It is equivalent to drawing a normal number on log-transformed axes and back transforming.
$$z \sim \textrm{lognormal}\left(\mu_{\log \alpha_i}, \sigma_{\log \alpha_i}\right)$$
  - model for $\mu_{\log \alpha_i}$ and $\sigma_{\log \alpha_i}$ stays the same, with priors being mean and variance of log-transformed values.
2.  Estimate cumulative hazard $\Lambda$ as
$$\Lambda = -T \, \left(\alpha_0\, \alpha_1 , \rho_s^{\alpha_2} \exp(\beta_0\, \beta_1 , \rho_s^{\beta_2}  * G) + \gamma_0\, \gamma_1 , \rho_s^{\gamma_2}\right)$$
3. Increment log probability density by appropriate amount.
  - If $y=0$ we have $$\Pr(y=0) = 1 - \Pr(y=1) = 1 - (1- \exp(-\Lambda) = \exp(-\Lambda),$$ so the log probability density is $$\log \Pr(y=0) = \log(\exp(-\Lambda)) = -\Lambda$$.
 - If $y=1$ we have $\Pr(y=1) = 1 - \exp(-\Lambda)$, so the log probability density is $$\log \Pr(y=1) = \log(1-\exp(-\Lambda)) = \textrm{log1m\_exp}(-\Lambda)$$.

## Analysis plan

Step 1 - define a general structure of model we think will capture data, on basis of prior knowledge [DONE]
Step 2 - assuming species effects on all pars, ask which growth meausre best predicts data, compare 2 growth rates using cross validation
Step 3 - fit full model with trait effects, look at effect sizes of WD on a, b and c
Step 4 - based on 3, choose some subsets of models, compare these using both bayes factor (most parsimonious) and predictive capacity


\pagebreak




\pagebreak


## Results
**Prelim results:**

**DBH_DT**
Wood density effects on alpha, beta and gamma:
*alpha* - no significant effect
*beta* - positive effect (increases decay rate)
*gamma* - negative effect (decreases hazard rate)

**BASAL_DT**
Wood density effects on alpha, beta and gamma:
*alpha* - no significant effect
*beta* - positive effect (increases decay rate)
*gamma* - marginal negative effect (not significant @ p <0.05)

**Best growth measure**
Preliminary it appears to be dbh_dt as it has a higher log likelihood.

\pagebreak



# Figures

```{r model description, fig.height=6,fig.width=6, echo=FALSE}
par(mfrow=c(2,1), mar=c(3,4,1,1), xaxs='i')
       curve(exp(1 -exp(-0.3) * x) + exp(-4), xlim=c(0,12), ylim=c(0,5),
             xlab=NA, ylab='Hazard rate',xaxt='n', col='red')
       arrows(x0 = 2, y0 = 2.7, x1 = 0 ,y1 = 2.7)
       text(3.1,2.7, 'alpha')
       arrows(x0 = 4, y0 = 1.5, x1 = 1.5 ,y1 = 1)
       text(4.5,1.7, 'beta')
       arrows(x0 = 11, y0 = 0.8, x1 = 11 ,y1 = 0.018)
       text(11,1, 'gamma')
       axis(1, at=0, labels= 0)

       curve((1-exp(-(exp(1 - exp(-0.3) * x) + exp(-4))))*100, xlim=c(0,12), ylim=c(0,100),
             xlab=NA, ylab='Pr(Mortality)/yr', xaxt='n', col='red')
       text(3.1,93, 'alpha')
       arrows(x0 = 2, y0 = 93, x1 = 0 ,y1 = 93)
       text(6,30, 'beta')
       arrows(x0 = 5, y0 = 30, x1 = 3 ,y1 = 30)
       text(11,15, 'gamma')
       arrows(x0 = 11, y0 = 10, x1 = 11 ,y1 = exp(-4)*100)
       axis(1, at=0, labels= 0)
       mtext(side=1, 'Growth rate',line = 2)
```

**Fig 6:** Interpreting model parameters. Top = Hazard curves (i.e. instantaneous mortality rates); Bottom = Mortality probability/year based on estimated hazard rates. The intercept at zero growth is defined by $alpha$. The slope is defined by $beta$. The asymptote is defined by $\gamma$ and represents the growth-independent hazard.



## References